---
title: 奇异值分解 SVD
date: 2017-04-12 08:56:22
categories: ml
tags:
- svd
- math
---

奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。

<!-- more -->

# 线性变换的几何解释

首先，我们来看一个只有两行两列的简单矩阵。

$$\begin{equation}
\mathbf{M}=
\begin{bmatrix}
	2 & 1 \\\\
	1 & 2
\end{bmatrix}
\end{equation}$$

从几何的角度，矩阵可以描述为一个变换：用矩阵乘法将平面上的点$(x,y)$ 变换成另外一个点 $(2x+y,x+2y)$ :

$$\begin{equation}
	\begin{bmatrix}
	2 & 1 \\\\
	1 & 2 \end{bmatrix}
	\begin{bmatrix}
	x \\\\
	y \end{bmatrix}
	=
	\begin{bmatrix}
	2x+y \\\\
	x+2y \end{bmatrix}
	\nonumber
\end{equation}$$

这种变换的效果如下：

<img title="矩阵变换" src="/imgs/ML/SVD/svd1.png" style="display:block;margin:auto" />

不过这张图貌似也并没有能够简洁、清晰的描述出上述矩阵变换的几何效果。然而，如果我们把网格旋转45度，再观察一下。

<img title="矩阵变换" src="/imgs/ML/SVD/svd2.png" style="display:block;margin:auto" />

我们看到现在这个新的网格在某一方向上被拉伸了3倍。

如果我们有一个2*2的对称矩阵，可以证明，我们总是可以通过在平面上旋转网格，使得矩阵变换的效果恰好是在两个垂直的方向上对网格的拉伸或镜面反射。

即给定一个对称矩阵 $M$ ，我们可以找到一组正交向量 $v_i$ 使得 $Mv_i$ 等于 $v_i$ 和标量的乘积；那就是

$$\begin{equation}
	Mv_i = \lambda_i v_i
\end{equation}$$

这里$\lambda_i$ 是标量。从几何意义上讲，这意味着当$v_i$ 乘上矩阵 $M$ 时被简单地拉伸或者反射了。因为这个性质，我们称 $v_i$ 是 $M$ 的特征向量；标量 $\lambda_i$ 被称为特征值。一个可以被证明的重要的事实是：对称矩阵不同的特征值对应的特征向量是正交的。如果我们把对称矩阵的特征向量和网格对齐，那么矩阵对网格的拉伸或反射的方式，与矩阵对特征向量的拉伸或反射的方式，两者是完全一致的。


# 奇异值分解

2\*2矩阵奇异值分解的几何实质是：对于任意2*2矩阵，总能找到某个正交网格到另一个正交网格的转换与矩阵变换相对应。

用向量解释这个现象：选择适当的正交的单位向量 $v_1$ 和$v_2$ ，向量 $Mv_i$ 和 $Mv_2$ 也是正交的。

<img title="矩阵变换" src="/imgs/ML/SVD/svd3.png" style="display:block;margin:auto" />

用 $u_1$ 和 $u_2$ 来表示 $Mv_1$ 和 $Mv_2$ 方向上的单位向量。 $Mv_1$ 和 $Mv_2$ 的长度用$\sigma_1$ 和  $\sigma_2$ 来表示——量化了网格在特定方向上被拉伸的效果。 $\sigma_1$ 和  $\sigma_2$ 被称为 $M$ 的奇异值。

由此，我们有

$$\begin{equation}
	Mv_1 = \sigma_1 u_1\\\\\
	Mv_2 = \sigma_2 u_2
\end{equation}$$

因为向量 $v_1$ 和 $v_2$ 是正交的单位向量，我们有

$$\begin{equation}
	x = (v_1 \cdot x)v_1 + (v_2 \cdot x)v_2
\end{equation}$$

$v_1 \cdot x$ 为单位向量与向量的内积，表示向量在该单位向量方向上的投影。

则有：

$$\begin{eqnarray}
	Mx &=& (v_1 \cdot x)Mv_1 + (v_2 \cdot x)Mv_2 \\\\
	&=& (v_1 \cdot x)\sigma_1 u_1 + (v_2 \cdot x)\sigma_2 u_2
\end{eqnarray}$$

注意点积（标量）可以用向量的转置来计算:

$$\begin{equation}
	v \cdot x = v^T x
\end{equation}$$

又有：

$$\begin{equation}
	Mx = u_1\sigma_1 v_1^T x + u_2\sigma_2 v_2^T x \\\\
	M = u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T
\end{equation}$$

即有：
$$\begin{eqnarray}
    M &=&
    \left[\begin{matrix}
    u_1 & u_2
    \end{matrix}\right]
    \left[\begin{matrix}
    \sigma_1 & \\\\
    &\sigma_2
    \end{matrix}\right]
    \left[\begin{matrix}
    v_1 & v_2
    \end{matrix}\right]^T \\\\
    &=&
    \left[\begin{matrix}
    \sigma_1u_1 & \sigma_2u_2
    \end{matrix}\right]
    \left[\begin{matrix}
    v_1^T \\\\ v_2^T
    \end{matrix}\right] \\\\
    &=&
    u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T
\end{eqnarray}$$

将上式写成矩阵相乘的形式有：

$$\begin{equation}
	M = UΣV^T
\end{equation}$$


这里 $U$ 是列向量 $u_1$ 和 $u_2$ 组成的矩阵，$Σ$ 是非零项为 $\sigma_1$ 和 $\sigma_2$ 的对角矩阵， $V$ 是列向量 $v_1$ 和 $v_2$ 组成的矩阵。

则有：

$$\begin{equation}
	MM^T=U(ΣΣ^T)U^T \\\\
	M^T M = V(Σ^T Σ)V^T
\end{equation}$$


关系式的右边描述了关系式左边的特征值分解。于是：

- $U$ 的列向量（左奇异向量）是 $MM^T$ 的特征向量。
- $V$ 的列向量（右奇异向量）是 $M^T M$ 的特征向量。
- $Σ$ 的非零对角元（非零奇异值）是 $MM^T$ 或者 $M^T M$ 的非零特征值的平方根。

上面描述了怎样将矩阵 $M$ 分解成三个矩阵的乘积： $V$ 描述了原始空间中的正交基， $U$ 描述了相关空间的正交基， $Σ$ 描述了 $V$ 中的向量变成 $U$ 中的向量时被拉伸的倍数。


# 举例

## SVD分解

对以下矩阵进行SVD分解：

$$\begin{equation}
C=\begin{pmatrix}
1 & -1 \\\\
0 & 1\\\\
1 & 0\\\\
-1 & 1
\end{pmatrix}
\end{equation}$$

计算$CC^T$矩阵的值如下：

$$\begin{equation}
CC^T=\begin{pmatrix}
1 & -1 \\\\
0 & 1\\\\
1 & 0\\\\
-1 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 1 & -1\\\\
-1 & 1 & 0 & 1
\end{pmatrix}
=
\begin{pmatrix}
2 & -1 & 1 & -2\\\\
-1 & 1 & 0 & 1\\\\
1 & 0 & 1 & -1\\\\
-2 & 1 & -1 & 2\\\\
\end{pmatrix} \\\\
\end{equation}$$

对以上结果求解特征值和特征向量为：

$$\begin{equation}
\lambda_1 = 5,p_1=\begin{pmatrix} -0.63  &  0.32  &  -0.32  &  0.63 \end{pmatrix}^T\\\\
\lambda_2 = 1,p_2=\begin{pmatrix} -0.00  &  -0.71  &  -0.71  &  0.00 \end{pmatrix}^T\\\\
\lambda_3 = 0,p_3=\begin{pmatrix} -0.77  &  -0.26  &  0.26  &  -0.52 \end{pmatrix}^T\\\\
\lambda_4 = 0,p_4=\begin{pmatrix} 0.02  &  -0.57  &  0.57  &  0.59 \end{pmatrix}^T
\end{equation}$$

计算$C^TC$矩阵的值如下：

$$\begin{equation}
C^TC=
\begin{pmatrix}
1 & 0 & 1 & -1\\\\
-1 & 1 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & -1 \\\\
0 & 1\\\\
1 & 0\\\\
-1 & 1
\end{pmatrix}
=
\begin{pmatrix}
3 & -2\\\\
-2 & 3
\end{pmatrix}
\end{equation}$$

对以上结果求解特征值和特征向量为：

$$\begin{equation}
\lambda_1 = 5,p_1=\begin{pmatrix} 0.71  &  -0.71 \end{pmatrix}^T\\\\
\lambda_2 = 1,p_2=\begin{pmatrix} 0.71  &  0.71 \end{pmatrix}^T
\end{equation}$$

即，矩阵$C$分解之后有
$$\begin{equation}
U=\begin{pmatrix}
-0.63 & 0.00 \\\\
0.32 & -0.71 \\\\
-0.32 & -0.71 \\\\
0.63 & -0.00
\end{pmatrix}\\\\
\sqrt{S}=\begin{pmatrix}
2.236 & 0\\\\
0 & 1
\end{pmatrix}\\\\
V=\begin{pmatrix}
0.7071 & 0.7071\\\\
-0.7071 & 0.7071
\end{pmatrix}
\end{equation}$$

则有：
$$\begin{equation}
C=U\sqrt{S}V^T=\begin{pmatrix}
-0.63 & 0.00 \\\\
0.32 & -0.71 \\\\
-0.32 & -0.71 \\\\
0.63 & -0.00
\end{pmatrix}
\begin{pmatrix}
2.236 & 0\\\\
0 & 1
\end{pmatrix}
\begin{pmatrix}
0.7071 & 0.7071\\\\
-0.7071 & 0.7071
\end{pmatrix}^T
=
\begin{pmatrix}
-1.00 & 1.00 \\\\
0.00 & -1.00 \\\\
-1.00 & -0.00 \\\\
1.00 & -1.00
\end{pmatrix}
\end{equation}$$

以上分解之后的矩阵的符号相反不影响。


## 数据压缩

奇异值分解可以高效的表示数据。例如，假设我们想传送下列图片，包含15*25个黑色或者白色的像素阵列。

<img title="例1" src="/imgs/ML/SVD/svd_example1.png" style="display:block;margin:auto" />

因为在图像中只有三种类型的列（如下），它可以以更紧凑的形式被表示。

<img title="例2" src="/imgs/ML/SVD/svd_example2.png" style="display:block;margin:auto" />

我们用15*25的矩阵来表示这个图像，其中每个元素非0即1，0表示黑色像素，1表示白色像素。如下所示，共有375个元素。


<img title="例3" src="/imgs/ML/SVD/svd_example3.png" style="display:block;margin:auto" />

如果对M进行奇异值分解的话，我们只会得到三个非零的奇异值。

$$\begin{equation}
	\sigma_1 = 14.72 \\\\
	\sigma_2 = 5.22 \\\\
	\sigma_3 = 3.31 \\\\
\end{equation}$$

因此，矩阵可以如下表示

$$\begin{equation}
	M = u_1\sigma_1 v_1^T + u_2\sigma_2 v_2^T + u_3\sigma_3 v_3^T
\end{equation}$$

我们有三个包含15个元素的向量 $v_i$ ，三个包含25个元素的向量 $u_i$ ，以及三个奇异值 $\sigma_i$ 。这意味着我们可以只用123个数字就能表示这个矩阵而不是出现在矩阵中的375个元素。在这种方式下，我们看到在矩阵中有3个线性独立的列，也就是说矩阵的秩是3。

# 参考文献
>[奇异值分解（We Recommend a Singular Value Decomposition）](http://www.flickering.cn/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/2015/01/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%88we-recommend-a-singular-value-decomposition%EF%BC%89//)
